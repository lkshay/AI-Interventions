# -*- coding: utf-8 -*-
"""Classification using Naive Bayes Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cTT7DQZuMZO0ge1YBg3UpBD4k32_KbdI

# Dataset

Method 1: You can download dataset from Keras. In this dataset the words are replaced by a unique number. According to Keras' website, "Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer "3" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: "only consider the top 10,000 most common words, but eliminate the top 20 most common words".

As a convention, "0" does not stand for a specific word, but instead is used to encode any unknown word."
"""

from keras.datasets import imdb
(x_train, y_train), (x_test, y_test) = imdb.load_data(path="imdb.npz",
                                                      num_words=None,
                                                      skip_top=0,
                                                      maxlen=None,
                                                      seed=115,
                                                      start_char=1,
                                                      oov_char=2,
                                                      index_from=3)

x_train.shape

# print((x_train[5]))
# print((y_train[5]))

print(max(x_train))

def list_list_max(listoflists):
  max_ = 0
  for lis in listoflists:
    max_ = max(lis) if(max(lis) >= max_) else max_
  
  return max_

num_distinct_words = list_list_max(x_train)
# print(list_list_max(x_train))
    

# count the total number of words in positive and negative reviews

# pos_word_count = 0
# neg_word_count = 0

# for i in range(len(x_train)):
#   if(y_train[i] == 0):
#     neg_word_count += len(x_train[i])
#   else:
#     pos_word_count += len(x_train[i])
# print(pos_word_count,neg_word_count)

c_pos = 0
c_neg = 0

c_pos = list(y_train).count(1)
c_neg = list(y_train).count(0)

import itertools

pos_doc_list = []
neg_doc_list = []


for i in range(len(x_train)):
  if(y_train[i] == 0):
    neg_doc_list.append(x_train[i])
  else:
    pos_doc_list.append(x_train[i])

# list of all positive combined into one and, same for negative lists
pos_doc_list = list(itertools.chain.from_iterable(pos_doc_list))
neg_doc_list = list(itertools.chain.from_iterable(neg_doc_list))

# Number of total words with (repeated) in both positive and negative spaces
pos_word_count = len(pos_doc_list)
neg_word_count = len(neg_doc_list)

import numpy as np
# word_pos_space_score = [0]*num_distinct_words#np.zeros(num_distinct_words)
# word_neg_space_score = [0]*num_distinct_words#np.zeros(num_distinct_words)


# print(pos_word_count,neg_word_count)

# # for i in range(num_distinct_words):
# #   word_pos_space_score[i] = pos_doc_list.count(i)
# #   word_neg_space_score[i] = neg_doc_list.count(i)


  
  
# # check for every word how man times it is positive space and negative space
# # import numpy as np
# # word_pos_space_score = np.zeros(num_distinct_words)
# # word_neg_space_score = np.zeros(num_distinct_words)

# # for i in range(num_distinct_words):
# #   for j in range(len(x_train)):
# #     if(y_train[j] == 0):
# #       word_neg_space_score[i] = x_train[j].count(i)
# #     else:
# #       word_pos_space_score[i] = x_train[j].count(i)

import math

# word_pos_space_score = [( pos_doc_list.count(i)) for i in range(num_distinct_words)]
# for i in range(num_distinct_words):
#   word_pos_space_score[i] = pos_doc_list.count(i)
#   word_neg_space_score[i] = neg_doc_list.count(i)


# check for every word how many times it is positive space and negative space
nums_pos,word_pos_space_score = np.unique(pos_doc_list, return_counts=True)
nums_neg,word_neg_space_score = np.unique(neg_doc_list, return_counts=True)
pos_dict = dict(zip(nums_pos,(word_pos_space_score+1)/(pos_word_count+len(nums_pos))))# add 1 to the score of every word to make the score of zero occurrance also 1
neg_dict = dict(zip(nums_neg,(word_neg_space_score+1)/(neg_word_count+len(nums_neg))))# """"""""""""""""""

train_score = [0]*len(x_train)
count = 0
for doc in x_train:
  
  prod_pos = 0
  prod_neg = 0
  
  for word in doc:

    prod_pos += math.log((pos_dict.get(word,1/(pos_word_count+len(nums_pos)))),10) #return default value from if number is not in document

    prod_neg += math.log((neg_dict.get(word,1/(neg_word_count+len(nums_neg)))),10) #return default value from if number is not in document
    
    train_score[count] = 1 if(prod_pos > prod_neg) else 0
    
    if(train_score[count] == y_train[count]):
      train_score[count] = 1
    else:
      train_score[count] = 0
  count += 1
      
train_accuracy = train_score.count(1)/len(x_train)


test_score = [0]*len(x_test)
count = 0
for doc in x_test: 
  prod_pos = 1
  prod_neg = 1
  for word in doc:
    prod_pos += math.log((pos_dict.get(word,1/(pos_word_count+len(nums_pos)))),10) # return default value if number is not in document
    prod_neg += math.log((neg_dict.get(word,1/(pos_word_count+len(nums_neg)))),10) # return default value if number is not in document
    
    test_score[count] = 1 if(prod_pos > prod_neg) else 0
    
    if(test_score[count] == y_test[count]):
      test_score[count] = 1
    else:
      test_score[count] = 0
  count += 1
      
test_accuracy = test_score.count(1)/len(x_test)


print("Classification: ",)
print("On training data: ",train_accuracy*100,"%")
print("On test data: ",test_accuracy*100,"%")

print("Misclassification: ",)
print("On training data:", 100 - train_accuracy*100,"%")
print("On test data:", 100 - test_accuracy*100,"%")

"""Method 2: You can download the original dataset with readible reviews. Please go to: 
http://ai.stanford.edu/~amaas/data/sentiment/

and download "Large Movie Review Dataset v1.0."

There are many different methods to upload dataset to colab. 
One method is to download the dataset to your local computer, then upload it to colab and then unzip it. Please see the code below:
"""

from google.colab import files

uploaded = files.upload()
!tar xzvf aclImdb_v1.tar.gz >/dev/null

(x_train, y_train), (x_test, y_test) = imdb.load_data(path="imdb.npz",
                                                      num_words=10,
                                                      skip_top=0,
                                                      maxlen=None,
                                                      seed=113,
                                                      start_char=1,
                                                      oov_char=2,
                                                      index_from=3)

print(x_train)